{
  "cells": [
    {
      "metadata": {
        "id": "Y7z0kV0f72v4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": " Purpose: Create CSV files and plots by states in U.S. with historical and forecast for both cases and deaths of COVID-19. In particular, this code demonstrates the use of extend_the_curve()\n\n(C) COPYRIGHT NOTICE\n\nAll or portions of the documentation and software included in this software\ndistribution from AFLCMC/HNII are copyrighted by Stephen Bique, who has assigned\nAll Rights for those portions to AFLCMC/HNII.  Outside the USA, AFLCMC/HNII has\ncopyright on some or all of the software developed for AFLCMC/HNII. Any files may\ncontain specific copyright notices and those notices must be retained in any derived\nwork.\n\nAFLCMC/HNII LICENSE\n\nAFLCMC/HNII may grant permission for redistribution and use in source and binary\nforms, with or without modification, of this software and documentation\ncreated for AFLCMC/HNII provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n3. All advertising materials mentioning features or use of this software\n   must display the following acknowledgements:\n\n   This product includes software developed for AFLCMC/HNII.\n\n4. Neither the name of AFLCMC/HNII nor the names of its contributors\n   may be used to endorse or promote products derived from this software\n   without specific prior written permission.\n\nTHE SOFTWARE PROVIDED BY AFLCMC/HNII IS PROVIDED BY AFLCMC/HNII AND CONTRIBUTORS\n``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL AFLCMC/HNII OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nThe views and conclusions contained in the software and documentation\nare those of the authors and should not be interpreted as representing\nofficial policies, either expressed or implied, of AFLCMC/HNII.\n\n"
    },
    {
      "metadata": {
        "id": "sESzRDBh_grW",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "import multiprocessing as mp\nagents = mp.cpu_count()\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXsW1hL1jgD3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Define desired folder location for output files"
    },
    {
      "metadata": {
        "id": "M_SYK53_ji4-",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "folder = ''  # Default folder is current folder",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdXWKoGbAKvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Append the directory to your python path using sys"
    },
    {
      "metadata": {
        "id": "CJVbYGA1juhW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Fetch data"
    },
    {
      "metadata": {
        "id": "mm6cVdMcj2K4",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nurl_cases = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\nurl_deaths = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n\ndf_cases = pd.read_csv(url_cases, error_bad_lines=True)\ndf_deaths = pd.read_csv(url_deaths, error_bad_lines=True)\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-N4XMovj_yT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Save start time for performance optimizations"
    },
    {
      "metadata": {
        "id": "-iRJfCYNT8oG",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\nstart_time = time.time()",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WzHFgXGvkMIa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Some abbreviations and names for 'Other' states:\n'AS' American Samoa\n'FM' Federated States of Micronesia\n'GU' Guam\n'MH' U.S.Minor Outlying Islands\n'MP' 'Marshall Islands\n'UM' Northern Mariana Island\n'PW' Palau\n'VI' U.S.Virgin Islands'\n\nStates for which we collect data:"
    },
    {
      "metadata": {
        "id": "l4U4I-iYlGA-",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "states = ['Alabama','Alaska','Arkansas','Arizona', 'California','Colorado','Connecticut','Delaware','District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_xA6vvGlUn-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Get length of rows:"
    },
    {
      "metadata": {
        "id": "aO8aNZ_XlXhJ",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "index_len_c = df_cases.shape[0]\nindex_len_d = df_deaths.shape[0]\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFZEfTxBlfK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Form Boolean arrays to select rows of dataframe:"
    },
    {
      "metadata": {
        "id": "dtdMT1cWliyP",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "rows_cases = []\nrows_deaths = []\nfor s in range(len(states)):\n    rows_cases.append(df_cases['Province_State'] == states[s])\n    rows_deaths.append(df_deaths['Province_State'] == states[s])",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yh_NTgHlloSA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Add 'Other' states:"
    },
    {
      "metadata": {
        "id": "FKmMiphFlpjY",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "rows_cases.append(pd.Series([not(i in states) for i in df_cases['Province_State']]))\nrows_deaths.append(pd.Series([not(i in states) for i in df_deaths['Province_State']]))\nstates.append('Other')",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zYLXM1zYl0tH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Insert total for all states:"
    },
    {
      "metadata": {
        "id": "MFIIdxIml5Vn",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "rows_cases.insert(0, pd.Series([True] * index_len_c))\nrows_deaths.insert(0, pd.Series([True] * index_len_d))\nstates.insert(0, 'U.S.')\n\nnum_states = len(states)\n\nfrom datetime import datetime\n\ndates_c = []\ncases_dates = list(df_cases.columns)[11:]\nfor i in cases_dates:\n    dates_c.append(datetime.strptime(i, '%m/%d/%y').strftime('%m/%d/%Y'))\n\ndates_d = []\ndeaths_dates = list(df_deaths.columns)[12:]\nfor i in deaths_dates:\n    dates_d.append(datetime.strptime(i, '%m/%d/%y').strftime('%m/%d/%Y'))\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lwo0omFNmP2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Compute partitions and ranges for the parallel agents to divide and conquer"
    },
    {
      "metadata": {
        "id": "uR4SZUqwmR9u",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "if agents > num_states:\n    agents = num_states # Can't distribute rows to more than rows agents",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NFmz9AiXmdLX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Generate partitions to divide sequence of num_items among num_workers processes:"
    },
    {
      "metadata": {
        "id": "DkKN6T2wmeeY",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def Partition(num_items, num_workers):\n    partition = []  # list of pairs containing starting offsets and size [[offset0,size0],[offset1,size1],...]\n    ranges = []  # list of pairs containing ranges [[offset0, last_plus_one0],[offset1, last_plus_one1],...]\n    \n    row_chunksize = num_items // num_workers\n    row_chunksize1 = row_chunksize + 1\n    size = num_items - num_workers * row_chunksize\n    last_offset = 0\n    if size != 0:\n        for s in range(size):\n            partition.append([last_offset, row_chunksize1])\n            next_offset = last_offset + row_chunksize1\n            ranges.append([last_offset, next_offset])\n            last_offset = next_offset\n            \n    for s in range(num_workers - size):\n        partition.append([last_offset, row_chunksize])\n        next_offset = last_offset + row_chunksize\n        ranges.append([last_offset, next_offset])\n        last_offset = next_offset\n    \n    return partition, ranges\n\nrow_partition, row_ranges = Partition(num_states,agents)\n",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYqe4R7Qw2aZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Compute the sums per state for the number of cases and deaths"
    },
    {
      "metadata": {
        "id": "rcmWqxeyw600",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def update_sums_c(pair):\n    result = []\n    for s in range(pair[1]):\n        li = []\n        for i in cases_dates:\n            row = pair[0]+s\n            li.append(df_cases[rows_cases[row]][i].sum())\n        result.append(li)\n    return result\n\ndef update_sums_d(pair):\n    result = []\n    for s in range(pair[1]):\n        li = []\n        for i in deaths_dates:\n            row = pair[0]+s\n            li.append(df_deaths[rows_deaths[row]][i].sum())\n        result.append(li)\n    return result\n\nimport numpy as np\n\nwith mp.Pool(agents) as p:\n    cases_states = p.map(update_sums_c,row_partition)\ncases_states = np.row_stack(cases_states)\n\nwith mp.Pool(agents) as p:\n    deaths_states = p.map(update_sums_d,row_partition)\ndeaths_states = np.row_stack(deaths_states)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pj2Osnzweiq8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Create pandas DataFrames with states as rows and dates as columns"
    },
    {
      "metadata": {
        "id": "4Ug4GpMMelIP",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "dfc = pd.DataFrame(cases_states, columns=dates_c, index=states)\ndfd = pd.DataFrame(deaths_states, columns=dates_d, index=states)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vy2mqE1WPWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Get length of columns"
    },
    {
      "metadata": {
        "id": "hvGhilJDWUtd",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "col_len_c = dfc.shape[1]\ncol_len_d = dfd.shape[1]",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rDPvqDeWaNP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Sort the dataframe by the worst as of latest data"
    },
    {
      "metadata": {
        "id": "nz5tKHI9WbVE",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_c = dfc.sort_values(dfc.columns[-1], ascending = False)\ndf_d = dfd.sort_values(dfd.columns[-1], ascending = False)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xV-fmO-WWoe3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Save last date for observations"
    },
    {
      "metadata": {
        "id": "YQsi8YIOWuQ-",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "last_date_c = dates_c[-1].replace('/', '-')\nlast_date_d = dates_d[-1].replace('/', '-')",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3G_lcZ8FW0Wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Forecast\n\nSpecify number of days for which to forecast; reasonable to expect that forecast (number of days ahead) cannot be larger than the number of days upon which forecast is based"
    },
    {
      "metadata": {
        "id": "6e9EIXbfXFyE",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "from datetime import datetime\nfrom datetime import timedelta\n\nimport numpy as np\n\ndays_ahead_c = 14\nstart_c = dates_c[-1]\nstart_date_c = datetime.strptime(start_c, '%m/%d/%Y')\nfor i in range(days_ahead_c):\n    dates_c.append((start_date_c + timedelta(days=i + 1)).strftime('%m/%d/%Y'))\nfuture_forecast_c = np.array([float(x) for x in range(col_len_c + days_ahead_c)])\n\ndays_ahead_d = 14\nstart_d = dates_d[-1]\nstart_date_d = datetime.strptime(start_d, '%m/%d/%Y')\nfor i in range(days_ahead_d):\n    dates_d.append((start_date_d + timedelta(days=i + 1)).strftime('%m/%d/%Y'))\nfuture_forecast_d = np.array([float(x) for x in range(col_len_d + days_ahead_d)])\n",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n3b6BmbNqaE",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "\ndef make_forecast_c(blocks):\n    import numpy as np\n    from extend_the_curve import extend_the_curve\n    return np.row_stack(np.asarray([extend_the_curve(row,days_ahead_c,returns=\"step\") for row in blocks]))\n\ndef make_forecast_d(blocks):\n    import numpy as np\n    from extend_the_curve import extend_the_curve\n    return np.row_stack(np.asarray([extend_the_curve(row,days_ahead_d,returns=\"step\") for row in blocks]))\n\nblocks_c = [df_c.iloc[ pair[0]:pair[1], :].values for pair in row_ranges] #.values should be replaced by to_numpy() for newer versions of pandas\nwith mp.Pool(agents) as p:\n    cases_forecast = p.map(make_forecast_c,blocks_c)\ncases_forecast = np.row_stack(cases_forecast)\ndfc = pd.DataFrame(np.concatenate((df_c.values,cases_forecast),axis=1), columns=dates_c, index=df_c.axes[0].tolist()) #tolist should be replaced by to_list\n\n# model forecasting for number of deaths\nblocks_d = [df_d.iloc[ pair[0]:pair[1], :].values for pair in row_ranges] #.values should be replaced by to_numpy() for newer versions of pandas\nwith mp.Pool(agents) as p:\n    deaths_forecast =  p.map(make_forecast_d,blocks_d)\ndeaths_forecast = np.row_stack(deaths_forecast)\ndfd = pd.DataFrame(np.concatenate((df_d.values,deaths_forecast),axis=1), columns=dates_d, index=df_d.axes[0].tolist())#tolist should be replaced by to_list\n",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVJC-SnJQ7Cd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "# Print only computation time\n"
    },
    {
      "metadata": {
        "id": "QkVWFqC9Q-FQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc5f9044-ae80-49d8-8e0b-65fc198e912d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"time elapsed prior to plotting: {:.2f} min\".format((time.time() - start_time)/60))",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "time elapsed prior to plotting: 4.71 min\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JQp6KRF2RAZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "Plot results and save CSV files"
    },
    {
      "metadata": {
        "id": "k61UO7TJRQVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80e7f0b2-ade7-4fd2-97e9-aa3806c61e06",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def make_plot( df, ax, markers, freq, xlen, days, title, ylabel ):\n\n    #  Add markers\n    for i, line in enumerate(ax.get_lines()):\n        line.set_marker(markers[i])\n\n    # Set frequency for labels on x-axis\n    frequency = freq\n\n    # Calculate the locations of the corners of the figure\n    padding = 1.5  # intervals on each side\n    left = xlen + padding * float(2 * frequency)  # number of dates that fit in width of frame\n    left = (left - float(days - 1) - padding * float(frequency)) / left\n    width = 1.0 - left\n    bottom, height = 0.0, 1.0\n    right, top = 1.0, 1.0\n\n    import matplotlib.patches as patches\n\n    topline = ax.axis()[3]\n    leftline = ax.axis()[1] * left\n\n\n    # Historical section\n    p = patches.Rectangle(\n        (0, bottom),left,height,fill=True,transform=ax.transAxes,facecolor='gray',edgecolor='silver',clip_on=False,alpha=0.1\n    )\n    ax.add_patch(p)\n    ax.text(leftline *.96,topline,'Historical',style='italic',fontsize=12, fontweight='bold',ha=\"right\",va=\"top\",bbox=dict(boxstyle='square,pad=0.2',ec='none', fc='white'))\n\n    # Forecast section\n    p = patches.Rectangle(\n        (left,bottom),width,height,fill=True,transform=ax.transAxes,fc='yellow',ec='yellowgreen',clip_on=False,alpha=0.1\n    )\n    ax.add_patch(p)\n    ax.text(leftline * 1.01,topline,'Forecast',style='italic',fontsize=12,fontweight='bold',ha=\"left\",va=\"top\",bbox=dict(boxstyle='square,pad=0.2',ec='yellow',fc='white'))\n\n    left = xlen - float(days + 1)\n    right = float(days) + left\n\n    import pandas\n\n    # Print extreme points and a few interesting points in between\n\n    # Label last observation maximum (assume data is sorted)\n    max_val = df.values[0][-days - 1]\n    ax.plot(left,max_val,'o',markersize=10,markeredgewidth=2,markeredgecolor='black',fillstyle='none')\n    if (max_val > 99999.0):\n        label = K_string(max_val)\n    else:\n        label = C_string(max_val)\n    ax.annotate(label,(left,max_val),textcoords=\"offset points\",xytext=(-12,0),ha='right',fontsize=8,\n                va='center',backgroundcolor='w',xycoords='data',\n                bbox=dict(boxstyle='square,pad=0.1',fc='white', ec='white'),\n                arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\",color='white',alpha=0.75))\n    # Label date line\n    ax.axvline(x=left,color='silver',linewidth = 3,alpha=0.3)\n    bottomline = ax.axis()[2]\n    ax.text(left,bottomline,'{0:s}'.format(df.columns[-days - 1][:5]),style='italic',size=9,ha=\"center\",va=\"bottom\",bbox=dict(ec='silver', fc='white'))\n\n    # Label last observation minimum (assume data is sorted)\n    min_val = df.values[-1][-days - 1]\n    ax.plot(left, min_val, 'o', markersize=10, markeredgewidth=2, markeredgecolor='black', fillstyle='none')\n    if (min_val > 99999.0):\n        label = K_string(min_val)\n    else:\n        label = C_string(min_val)\n    ax.annotate(label,(left,min_val),textcoords=\"offset points\",xytext=(-12,0),ha='right',fontsize=8,\n                va='center',backgroundcolor='w',xycoords='data',\n                bbox=dict(boxstyle='square,pad=0.1',fc='white', ec='white'),\n                arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\",color='white',alpha=0.75))\n\n\n    # Label last prediction maximum value - N.B. prediction can and does change rankings\n    max_index = df.iloc[:,-1].values.argmax() # row containing maximum\n    max_value = df.values[max_index][-1]\n    ax.plot(right, max_value, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue',fillstyle='none')\n    if (max_value > 99999.0):\n        label = K_string(max_value)\n    else:\n        label = C_string(max_value)\n    ax.annotate(label,(right, max_value),textcoords=\"offset points\",xytext=(12,0),ha='left',fontsize=8,\n                va='center',color='blue',backgroundcolor='w',xycoords='data',\n                bbox=dict(boxstyle='square,pad=-0.07',fc='white', ec='yellow'),\n                arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\",color='white',alpha=0.75))\n    ax.axvline(x=right,color='gray',linewidth = 3,alpha=0.2)\n    ax.text(right,bottomline,'{0:s}'.format(df.columns[-1][:5]),style='italic',size=9,ha=\"center\",va=\"bottom\",bbox=dict(ec='silver', fc='white'))\n\n    # Label last prediction minimum value - N.B. prediction can and does change rankings\n    min_index = df.iloc[:,-1].values.argmin() # row containing maximum\n    min_value = df.values[min_index][-1]\n    ax.plot(right, min_value, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue',fillstyle='none')\n    if (min_value > 99999.0):\n        label = K_string(min_value)\n    else:\n        label = C_string(min_value)\n    ax.annotate(label,(right,min_value),textcoords=\"offset points\",xytext=(12,0),ha='left',fontsize=8,\n                va='center',color='blue',backgroundcolor='w',xycoords='data',\n                bbox=dict(boxstyle='square,pad=-0.07',fc='white', ec='yellow'),\n                arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\",color='white',alpha=0.75))\n\n    maximum = max(max_val,max_value)\n    nrows = len(df.index) # number of rows\n\n    if nrows > 1:\n        # Print other interesting points, but don't mess it up!\n\n        # Print second maximum\n\n        # Observed 2nd max based on current ranking\n        value_obs2 = df.values[1][-days - 1]\n        if (max_val - value_obs2) / maximum >= 0.0333:  # Skip if too close to observed maximum value\n            ax.plot(left, value_obs2, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue', fillstyle='none')\n            if (value_obs2 >= 1e6):\n                label = K_string(value_obs2)\n            else:\n                label = C_string(value_obs2)\n            ax.annotate(label, (left, value_obs2), textcoords=\"offset points\", xytext=(-12, 0), ha='right', fontsize=8,\n                        va='center', color='blue', backgroundcolor='w', xycoords='data',\n                        bbox=dict(boxstyle='square,pad=0.1', fc='white', ec='white'),\n                        arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", color='white', alpha=0.75))\n\n        # Predicted 2nd max based on current ranking, unless 2nd max is the predicted maximum\n        # We avoid sorting to find second max and look only what has happened to observed second value,\n        # and if it becomes the max, then we look what happened to previous max\n        if max_index == 1:\n            index2 = 0\n        else:\n            index2 = 1\n        value2 = df.values[index2][-1]\n        if (max_value - value2) / maximum >= 0.0333:  # Skip if too close to observed maximum value\n            ax.plot(right, value2, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue', fillstyle='none')\n            if (value2 >= 1e6):\n                label = K_string(value2)\n            else:\n                label = C_string(value2)\n            ax.annotate(label, (right, value2), textcoords=\"offset points\", xytext=(12, 0), ha='left', fontsize=8,\n                        va='center', color='blue', backgroundcolor='w', xycoords='data',\n                        bbox=dict(boxstyle='square,pad=-0.07', fc='white', ec='yellow'),\n                        arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", color='white', alpha=0.75))\n\n        # Label last 'median' values - N.B. prediction can and does change rankings (here, median refers to current ranking only)\n        # In fact, we are plotting an 'interesting point' and if this state has changed ranking, it is only more interesting.\n\n        # Observed median based on current ranking\n        median_index = nrows // 2  # row number that is the median\n        median_value_obs = df.values[median_index][-days - 1]\n        if ((median_value_obs - min_val) / maximum >= 0.0333) and (\n                abs(median_value_obs - value_obs2) / maximum >= 0.0333):  # Skip if too close to mediam value\n            ax.plot(left, median_value_obs, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue',\n                    fillstyle='none')\n            if (median_value_obs >= 1e6):\n                label = K_string(median_value_obs)\n            else:\n                label = C_string(median_value_obs)\n            ax.annotate(label, (left, median_value_obs), textcoords=\"offset points\", xytext=(-12, 0), ha='right',\n                        fontsize=8,\n                        va='center', color='blue', backgroundcolor='w', xycoords='data',\n                        bbox=dict(boxstyle='square,pad=0.1', fc='white', ec='white'),\n                        arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", color='white', alpha=0.75))\n\n        # Predicted median\n        median_value_pred = df.values[median_index][-1]\n        if (median_value_pred - min_value) / maximum >= 0.0333:  # Skip if too close to minimum value\n            ax.plot(right, median_value_pred, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue',\n                    fillstyle='none')\n            if (median_value_pred >= 1e6):\n                label = K_string(median_value_pred)\n            else:\n                label = C_string(median_value_pred)\n            ax.annotate(label, (right, median_value_pred), textcoords=\"offset points\", xytext=(12, 0), ha='left',\n                        fontsize=8,\n                        va='center', color='blue', backgroundcolor='w', xycoords='data',\n                        bbox=dict(boxstyle='square,pad=-0.07', fc='white', ec='yellow'),\n                        arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", color='white', alpha=0.75))\n\n        # Lastly, let's look the 'median' between 2nd max and median\n        # Observed\n        index = (median_index + 1) // 2\n        value = df.values[index][-days - 1]\n        if (abs(value_obs2 - value) / maximum >= 0.0333) and (\n                abs(median_value_obs - value) / maximum >= 0.0333):  # Skip if too close\n            ax.plot(left, value, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue', fillstyle='none')\n            if (value >= 1e6):\n                label = K_string(value)\n            else:\n                label = C_string(value)\n            ax.annotate(label, (left, value), textcoords=\"offset points\", xytext=(-12, 0), ha='right',\n                        fontsize=8,\n                        va='center', color='blue', backgroundcolor='w', xycoords='data',\n                        bbox=dict(boxstyle='square,pad=0.1', fc='white', ec='white'),\n                        arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", color='white', alpha=0.75))\n\n        # Predicted\n        value = df.values[index][-1]  # use same index\n        if (abs(value2 - value) / maximum >= 0.0333) and (\n                abs(median_value_pred - value) / maximum >= 0.0333):  # Skip if too close\n            ax.plot(right, value, 'o', markersize=10, markeredgewidth=2, markeredgecolor='blue',\n                    fillstyle='none')\n            if (value >= 1e6):\n                label = K_string(value)\n            else:\n                label = C_string(value)\n            ax.annotate(label, (right, value), textcoords=\"offset points\", xytext=(12, 0), ha='left',\n                        fontsize=8,\n                        va='center', color='blue', backgroundcolor='w', xycoords='data',\n                        bbox=dict(boxstyle='square,pad=-0.07', fc='white', ec='yellow'),\n                        arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", color='white', alpha=0.75))\n\n    # Add legend upper-left with medium font in ncol columns\n    if nrows > 20:\n        ncol = 3\n    elif nrows > 10:\n        ncol = 2\n    else:\n        ncol = 1\n    ax.legend(ax.get_lines(), df.index, prop={'size': 11}, loc='upper left', ncol=ncol,frameon=False)\n\n    # Add legend for 'Other' if it exists in dataframe\n    if (df.index == 'Other').any():\n        ax.text(0,ax.axis()[3]*.2,'Other: AS,FM,GU,MH,MP,UM,PW,VI', verticalalignment='bottom',horizontalalignment='left',color='black',fontsize=11)\n\n    # Add y-label\n    ax.set_ylabel(ylabel, fontsize=12)\n\n    # Set nice ticks on y-axis\n    from matplotlib.ticker import FuncFormatter\n    if (max_value >= 3000):\n        ax.yaxis.set_major_formatter(FuncFormatter(thousands))\n    else:\n        ax.yaxis.set_major_formatter(FuncFormatter(comma))\n\n    # Make nice labels on x-axes with rotation\n    from matplotlib.ticker import FixedLocator, FixedFormatter\n    ax.xaxis.set_major_locator(FixedLocator(np.arange(0, xlen, freq)))\n    ax.xaxis.set_major_formatter(FixedFormatter(df.columns[::freq]))\n    #ax.set_xticklabels(df.columns[::freq], fontsize=12)\n    for label in ax.get_xticklabels():\n        label.set_rotation(20)\n        label.set_horizontalalignment('right')\n\n    # Add title\n    ax.set_title(title, fontsize=14)\n\n\ndef subplots(t):\n    (df1,df2, fig, axes, markers1,markers2, freq1,freq2, xlen1,xlen2, days1,days2, title1,title2, ylabel1,ylabel2, file) = t\n\n    #########################\n    # First subplot\n\n    import pandas\n    ax  = df1.T.plot(ax=axes[0], lw=0.5, marker='o', markersize=5)\n\n    make_plot(df1, ax, markers1, freq1, xlen1, days1, title1, ylabel1 )\n\n    ###########################\n    # Second subplot\n\n    ax_ = df2.T.plot(ax=axes[1], lw=0.5, marker='s', markersize=5)\n\n    make_plot(df2, ax_, markers2, freq2, xlen2, days2, title2, ylabel2 )\n\n    fig.subplots_adjust(hspace=0.4)\n\n    # #########################\n    # Save plot\n\n    import matplotlib.pyplot as plt\n    fig.savefig(file, dpi=300)\n\n\n# create 'full' list of available markers\nmarks = ['o', 'v', '^', '<', '>', '1', '2', '3', '4', '8', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', 'P', 'X']\n\nmarkers_c = np.resize(marks, dfc.shape[0])\nmarkers_d = np.resize(marks, dfd.shape[0])\n\ndef K_string(x):\n    return '{0:,}K'.format(int(x * 1e-3))\n\ndef C_string(x):\n    return '{0:,}'.format(int(x))\n\n# Format functions for y-axis\ndef comma(x, pos):  # formatter function takes tick label and tick position\n    return '{:0,d}'.format(int(x))\n\ndef thousands(x, pos):\n    'The two args are the value and tick position'\n    return '%3dK' % (x * 1e-3)\n\n\n# Ignore initial dates when numbers are low, choose somewhere in 3/2-3/17\nfrom datetime import datetime\nfirst_c = (datetime.strptime('03/09/2020', '%m/%d/%Y').date() - datetime.strptime(dates_c[0], '%m/%d/%Y').date()).days\nfirst_d = (datetime.strptime('03/09/2020', '%m/%d/%Y').date() - datetime.strptime(dates_d[0], '%m/%d/%Y').date()).days\n\n# Specify number of axes in subplots to provide a better comparison of states\n\nimport matplotlib.pyplot as plt\n\n'''\nSet sizes for each plot\n\nUpon running the code, it is easy to see if there is a 'good' separation so that comparison is useful, and if not, \nadjust the numbers above accordingly or as desired. The idea is that all of the states are partitioned into four \ncategories from worst to best as follows\n\n                                 worst\n                                 Tier 1\n                                 Tier 2\n                                 best\n\nfor both the number of cases and the number of deaths, and only the first three are specified as the remaining is \nthen determined (since the total number is fixed).\n'''\nnum_worst_c = 12          # number of states to include in 'Worst' for cases\nnum_cases_tier1 = 23      # number of states to include in 'Tier 1' for cases\nnum_cases_tier2 = 11      # number of states to include in 'Tier 2' for cases\n\nnum_worst_d = 17          # number of states to include in 'Worst' for deaths\nnum_deaths_tier1 = 20     # number of states to include in 'Tier 1' for deaths\nnum_deaths_tier2 = 12     # number of states to include in 'Tier 2' for deaths\n\n# First two subplots consist of cumulative (US) for both cases and deaths\nfig0, axes0 = plt.subplots(2,num=1,figsize=(21,12), dpi=80)\nnum_axes0 = 0\nnum_axes0_ = 1\ndf0_1 = dfc.iloc[num_axes0:num_axes0_,first_c:]\ndf0_2 = dfd.iloc[num_axes0:num_axes0_,first_d:]\n\n# Next two subplots consist of worst num_worst states for both cases and deaths\nfig1, axes1 = plt.subplots(2,num=2,figsize=(21,26), dpi=80)\nnum_axes1 = num_axes0_\nnum_axes1_c = num_axes1 + num_worst_c\nnum_axes1_d = num_axes1 + num_worst_d\ndf1_1 = dfc.iloc[num_axes1:num_axes1_c,first_c:]\ndf1_2 = dfd.iloc[num_axes1:num_axes1_d,first_d:]\n\n# Next two subplots consists of next worst num_cases_tier1 and num_cases_tier2 states, resp., which is adjusted\n# depending on the apparent rankings, for cases only\n\nfig2, axes2 = plt.subplots(2,num=3,figsize=(21,20), dpi=80)\nnum_c_axes2  = num_axes1_c\nnum_c_axes2_ = num_c_axes2 + num_cases_tier1\nnum_c_axes3  = num_c_axes2_\nnum_c_axes3_ = num_c_axes3 + num_cases_tier2\ndf2_1 = dfc.iloc[num_c_axes2:num_c_axes2_,first_c:]\ndf2_2 = dfc.iloc[num_c_axes3:num_c_axes3_,first_c:]\n\n# Next two subplots consist of next worst num_deaths_tier1 and num_deaths_tier2 states, resp., which is adjusted\n# depending on the apparent rankings, for deaths only\nfig3, axes3 = plt.subplots(2,num=4,figsize=(21,20), dpi=80)\nnum_d_axes2  = num_axes1_d\nnum_d_axes2_ = num_d_axes2 + num_deaths_tier1\nnum_d_axes3  = num_d_axes2_\nnum_d_axes3_ = num_d_axes3 + num_deaths_tier2\ndf3_1 = dfd.iloc[num_d_axes2:num_d_axes2_,first_d:]\ndf3_2 = dfd.iloc[num_d_axes3:num_d_axes3_,first_d:]\n\n# Next two subplots consist of remaining best states for cases and deaths\nfig4, axes4 = plt.subplots(2,num=5,figsize=(21,20), dpi=80)\ndf4_1 = dfc.iloc[num_c_axes3_:,first_c:]\ndf4_2 = dfd.iloc[num_d_axes3_:,first_d:]\n\n# Set frequency for labels on x-axes\nfreq_c = (len(dates_c) - first_c + 31)// 32\nfreq_d = (len(dates_d) - first_d + 31)// 32\n\n# Compute length of x-axis\nxlen_c = float(len(dates_c) - first_c)\nxlen_d = float(len(dates_d) - first_d)\n\ntasks=((df0_1, df0_2, fig0, axes0, markers_c,markers_d,freq_c,freq_d,xlen_c,xlen_d,days_ahead_c,days_ahead_d,\n        'COVID-19 Cases in U.S. with Forecast',\n        'COVID-19 Deaths in U.S. with Forecast',\n        'Cases (thousands)','Deaths (thousands)',\n        folder+'COVID-19-US-Forecast-'+last_date_c+'.png'),\n       (df1_1, df1_2, fig1, axes1, markers_c, markers_d, freq_c, freq_d, xlen_c, xlen_d, days_ahead_c, days_ahead_d,\n        'COVID-19 Cases in U.S. for Worst States with Forecast',\n        'COVID-19 Deaths in U.S. for Worst States with Forecast',\n        'Cases (thousands)', 'Deaths (thousands)',\n        folder + 'COVID-19-Worst-States-with-Forecast-' + last_date_c + '.png'),\n       (df2_1,df2_2,fig2,axes2,markers_c,markers_c,freq_c,freq_c,xlen_c,xlen_c,days_ahead_c,days_ahead_c,\n        \"COVID-19 Cases in U.S. for Selected 'Tier 1' States with Forecast\", \"COVID-19 Cases in U.S. for Selected 'Tier 2' States with Forecast\",\n        'Cases (thousands)','Cases (thousands)',folder + 'COVID-19-Cases-for-Tier-States-' + last_date_c + '.png'),\n       (df3_1,df3_2,fig3,axes3,markers_d,markers_d,freq_d,freq_d,xlen_d,xlen_d,days_ahead_d,days_ahead_d,\n        \"COVID-19 Deaths in U.S. for Selected 'Tier 1' States with Forecast\",\n        \"COVID-19 Deaths in U.S. for Selected 'Tier 2' States with Forecast\",\n        'Deaths (thousands)','Deaths',folder+'COVID-19-Deaths-for-Tier-States-'+last_date_d+'.png'),\n       (df4_1,df4_2,fig4,axes4,markers_c,markers_d,freq_c,freq_d,xlen_c,xlen_d,days_ahead_c,days_ahead_d,\n        'COVID-19 Cases in U.S. for Best States with Forecast','COVID-19 Deaths in U.S. for Best States with Forecast',\n        'Cases (thousands)','Deaths',folder+'COVID-19-Best-States-with-Forecast-'+last_date_c+'.png'))\n\n\n# Make plots\n\nfor input in tasks:\n    subplots(input)\n\nprint(\"time elapsed including plotting: {:.2f} min\".format((time.time() - start_time)/60))\n\n# Save data sets to CSV files\n\ndfc.to_csv(folder + 'COVID-19-Cases-' + last_date_c + '.csv', mode ='w')\ndfd.to_csv(folder + 'COVID-19-Deaths-' + last_date_d + '.csv', mode ='w')\n\n# Show plot\nplt.show()",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "time elapsed including plotting: 7.73 min\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f50d24fb2b0>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f50d151a780>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f50d14e82e8>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f50d151a940>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f50d14ac6a0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "KThe0YJ9aZtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "af70c4e7-791f-4794-adf9-3fdfcd6c31ae",
        "trusted": true
      },
      "cell_type": "code",
      "source": "!zip \"`date +%m-%d-%Y`-COVID-19-forecast.zip\" *.csv *.png\n!rm *.csv *.png",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "  adding: COVID-19-Cases-08-16-2020.csv (deflated 67%)\n  adding: COVID-19-Deaths-08-16-2020.csv (deflated 75%)\n  adding: COVID-19-Best-States-with-Forecast-08-16-2020.png (deflated 17%)\n  adding: COVID-19-Cases-for-Tier-States-08-16-2020.png (deflated 10%)\n  adding: COVID-19-Deaths-for-Tier-States-08-16-2020.png (deflated 11%)\n  adding: COVID-19-US-Forecast-08-16-2020.png (deflated 23%)\n  adding: COVID-19-Worst-States-with-Forecast-08-16-2020.png (deflated 14%)\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "WGLS for COVID-19 colab v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}